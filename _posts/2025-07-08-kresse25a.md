---
title: Logic Gate Neural Networks are Good for Verification
abstract: Learning-based systems are increasingly deployed across various domains,
  yet the complexity of traditional neural networks poses significant challenges for
  formal verification. Unlike conventional neural networks, learned Logic Gate Networks
  (LGNs) replace multiplications with Boolean logic gates, yielding a sparse, netlist-like
  architecture that is inherently more amenable to symbolic verification, while still
  delivering promising performance. In this paper, we introduce a SAT encoding for
  verifying global robustness and fairness in LGNs. We evaluate our method on five
  benchmark datasets, including a newly constructed 5-class variant, and find that
  LGNs are both verification-friendly and maintain strong predictive performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kresse25a
month: 0
tex_title: Logic Gate Neural Networks are Good for Verification
firstpage: 90
lastpage: 103
page: 90-103
order: 90
cycles: false
bibtex_author: Kresse, Fabian and Yu, Emily and Lampert, Christoph H. and Henzinger,
  Thomas A.
author:
- given: Fabian
  family: Kresse
- given: Emily
  family: Yu
- given: Christoph H.
  family: Lampert
- given: Thomas A.
  family: Henzinger
date: 2025-07-08
address:
container-title: Proceedings of the International Conference on Neuro-symbolic Systems
volume: '288'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v288/main/assets/kresse25a/kresse25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
