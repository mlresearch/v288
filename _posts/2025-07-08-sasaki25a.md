---
title: 'Neurosymbolic Finite and Pushdown Automata: Improved Multimodal Reasoning
  versus Vision Language Models (VLMs)'
abstract: Multimodal large language models (LLMs), such as vision language models
  (VLMs), are powerful reasoning tools that have been shown to be capable of solving
  non-trivial tasks such as image and video reasoning, translation, and text generation.
  Alternatively, LLMs have also regularly been shown to have difficulties with trivial
  tasks like performing elementary mathematical reasoning problems and arithmetic.
  Significant effort has since gone towards directly addressing these tasks in order
  to show reasoning in LLMs. Despite an extensive training regimen that includes state-of-the-art
  hardware support and copious amounts of data, it is still straightforward to modify
  a, now, relatively trivial task such that the LLM again experiences a great deal
  of difficulty in solving the task. In this work, we focus on two tasks, image-based
  string acceptance and image-based arithmetic evaluation, for VLMs to solve that
  involve non-trivial multimodal reasoning and introduce a new neurosymbolic-based
  model of computation that can significantly outperform VLMs on them. We define two
  classes of neurosymbolic automata to address this problem, namely neurosymbolic
  finite automata (NSFA) and neurosymbolic pushdown automata (NSPDA). These neurosymbolic
  automata are able to model the image-based string acceptance and arithmetic evaluation
  tasks well, given their derivation from finite and pushdown automata for string
  acceptance and arithmetic evaluation. We show that state-of-the-art LLMs with multimodal
  reasoning capabilities are not only outperformed by neurosymbolic automata, but
  often fail to reason about the tasks altogether with the VLMs getting zero correct
  in the arithmetic evaluation task while the NSPDA demonstrates 88% accuracy with
  2 operands and a steady decline as the complexity of the expressions increased,
  as expected.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sasaki25a
month: 0
tex_title: 'Neurosymbolic Finite and Pushdown Automata: Improved Multimodal Reasoning
  versus Vision Language Models (VLMs)'
firstpage: 170
lastpage: 187
page: 170-187
order: 170
cycles: false
bibtex_author: Sasaki, Samuel and Manzanas Lopez, Diego and Johnson, Taylor T.
author:
- given: Samuel
  family: Sasaki
- given: Diego
  family: Manzanas Lopez
- given: Taylor T.
  family: Johnson
date: 2025-07-08
address:
container-title: Proceedings of the International Conference on Neuro-symbolic Systems
volume: '288'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v288/main/assets/sasaki25a/sasaki25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
