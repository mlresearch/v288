---
title: 'End-to-End Navigation with Vision-Language Models: Transforming Spatial Reasoning
  into Question-Answering'
abstract: We present VLMnav, an embodied framework to transform a Vision-Language
  Model (VLM) into an end-to-end navigation policy. In contrast to prior work, we
  do not rely on a separation between perception, planning, and control; instead,
  we use a VLM to directly select actions in one step. Surprisingly, we find that
  a VLM can be used as an end-to-end policy zero-shot, i.e., without any fine-tuning
  or exposure to navigation data. This makes our approach open-ended and generalizable
  to any downstream navigation task. We run an extensive study to evaluate the performance
  of our approach in comparison to baseline prompting methods. In addition, we perform
  a design analysis to understand the most impactful design decisions. Visual examples
  and code for our project can be found at jirl-upenn.github.io/VLMnav/.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: goetting25a
month: 0
tex_title: 'End-to-End Navigation with Vision-Language Models: Transforming Spatial
  Reasoning into Question-Answering'
firstpage: 22
lastpage: 35
page: 22-35
order: 22
cycles: false
bibtex_author: Goetting, Dylan and Singh, Himanshu Gaurav and Loquercio, Antonio
author:
- given: Dylan
  family: Goetting
- given: Himanshu Gaurav
  family: Singh
- given: Antonio
  family: Loquercio
date: 2025-07-08
address:
container-title: Proceedings of the International Conference on Neuro-symbolic Systems
volume: '288'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v288/main/assets/goetting25a/goetting25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
