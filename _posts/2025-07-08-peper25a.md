---
title: Four Principles for Physically Interpretable World Models
abstract: 'As autonomous systems are increasingly deployed in open and uncertain settings,
  there is a growing need for trustworthy neuro-symbolic world models that can reliably
  predict future high-dimensional observations. The learned latent representations
  in world models lack direct mapping to meaningful physical quantities and dynamics,
  limiting their utility and interpretability in downstream planning, control, and
  safety verification. In this paper, we argue for a fundamental shift from physically
  informed to physically interpretable world modelsâ€”and crystallize four principles
  that leverage symbolic knowledge to achieve these ends: (1) functionally organizing
  the latent space according to the physical intent, (2) learning aligned invariant
  and equivariant representations of the physical world, (3) integrating multiple
  forms and strengths of supervision into a unified training process, and (4) partitioning
  generative outputs to support scalability and verifiability. We experimentally demonstrate
  the value of each principle on two benchmarks. This paper opens several intriguing
  research directions to achieve and capitalize on full physical interpretability
  in learned world models.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peper25a
month: 0
tex_title: Four Principles for Physically Interpretable World Models
firstpage: 66
lastpage: 89
page: 66-89
order: 66
cycles: false
bibtex_author: Peper, Jordan and Mao, Zhenjiang and Geng, Yuang and Pan, Siyuan and
  Ruchkin, Ivan
author:
- given: Jordan
  family: Peper
- given: Zhenjiang
  family: Mao
- given: Yuang
  family: Geng
- given: Siyuan
  family: Pan
- given: Ivan
  family: Ruchkin
date: 2025-07-08
address:
container-title: Proceedings of the International Conference on Neuro-symbolic Systems
volume: '288'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v288/main/assets/peper25a/peper25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
