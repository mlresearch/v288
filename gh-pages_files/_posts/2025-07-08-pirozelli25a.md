---
title: A Study of Modus Ponens in Transformer Models
abstract: Transformer models are the backbone of modern natural language processing.
  However, whether they can truly perform logical reasoning remains uncertain. This
  paper examines transformers’ capacity for logical inference in a controlled setting,
  isolating a single rule—modus ponens—and eliminating confounding factors such as
  semantic knowledge and linguistic complexity. We systematically vary architectural
  components, specifically the number of attention heads and layers, to assess their
  impact on logical inference. Our results show that attention heads enhance information
  propagation, whereas deeper architectures accelerate convergence but also introduce
  potentially redundant parameters. While transformers successfully handle level-2
  inference tasks, their difficulties with higher-level and out-of-distribution problems
  suggest that they rely on heuristic “shortcuts” rather than explicit multi-step
  reasoning. Analysis of attention maps and ablation experiments indicates that these
  shortcuts function similarly to a matching-aggregation algorithm, where attention
  heads identify inference links, and the feed-forward network verifies if they form
  a valid chain. These findings highlight fundamental limitations in transformers’
  ability to perform structured logical reasoning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pirozelli25a
month: 0
tex_title: A Study of Modus Ponens in Transformer Models
firstpage: 293
lastpage: 315
page: 293-315
order: 293
cycles: false
bibtex_author: Pirozelli, Paulo and Cozman, Fabio G.
author:
- given: Paulo
  family: Pirozelli
- given: Fabio G.
  family: Cozman
date: 2025-07-08
address:
container-title: Proceedings of the International Conference on Neuro-symbolic Systems
volume: '288'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 8
pdf: https://raw.githubusercontent.com/mlresearch/v288/main/assets/pirozelli25a/pirozelli25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
